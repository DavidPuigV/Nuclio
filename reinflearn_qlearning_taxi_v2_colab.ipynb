{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinflearn-qlearning-taxi-v2-colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidPuigV/Nuclio/blob/main/reinflearn_qlearning_taxi_v2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjr3vlWrBhOG"
      },
      "source": [
        "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnBoQcviBhOH"
      },
      "source": [
        "# Reinforced Learning con Q-learning - El taxi autónomo\n",
        "\n",
        "El primer ejemplo de reinforced learning más en serio que lo del multi-armed bandit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MIMXy_-BtxT"
      },
      "source": [
        "## 1. Instalemos las librerias en Colab para tener gym y poder mostrar los videos grabados\n",
        "Más detalles en este notebook de ejemplo del equipo de Colab https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H#scrollTo=7wY4qZhPXotR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wwE6DEvBl__",
        "outputId": "1412f52b-b562-4e4a-d43a-ec0fea100db3"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (760 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 146425 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (1,103 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 148780 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsgpKLMIBhOI",
        "outputId": "b5c41a5f-107a-4c5d-f909-22a9205e7737"
      },
      "source": [
        "!pip install gym[atari]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v64O_k04CKqh",
        "outputId": "f5532488-1dc8-43b8-f275-60f6c4b8ccb9"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/05/6568620fed440941b704664b9cfe5f836ad699ac7694745e7787fbdc8063/PyVirtualDisplay-2.0-py2.py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.0\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Collecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfOcRdjJCNu-",
        "outputId": "f3fe0331-ffed-40b7-986d-2377306d5e2b"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f672c63b898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnD6VzNRCTx8"
      },
      "source": [
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v0Kre1RCZ-1"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYmBgBOzCkKf"
      },
      "source": [
        "Las dos siguientes funciones son para poder grabar en video los entornos de gym y mostrarlos\n",
        "\n",
        "Para activar el video, solo teneis que hacer \"env = wrap_env(env)\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO4yNfhLChi0"
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdXmbzKfDHoG"
      },
      "source": [
        "## 2. A por el entorno del Taxi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHqcYj06DHCS",
        "outputId": "527729a1-b214-4010-9ce6-d3105531bbfe"
      },
      "source": [
        "entorno = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "entorno.render()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM2kmvopDjUT"
      },
      "source": [
        "Esto se parece un poco a lo que hemos estado viendo en las slides, la única diferencia (que era así en la version v2 de Taxi) es que el taxi esta en la posición R, en vez de la (1,3) como teniamos pintado antes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBXsSKz7DyB3"
      },
      "source": [
        "El universo de la interfaz gym es <code> entorno </code>. Tenemos algunos métodos que iremos usando que os pueden ser útiles:\n",
        "\n",
        "<ul>\n",
        "  <li><code> entorno.reset </code> Pone el entorno en posición de fábrica, y nos devuelve un estado inicial random</li>\n",
        "  <li><code> entorno.step(action) </code> Paso del entorno tras un incremento de tiempo. Esto nos devuelve:\n",
        "  <ul>\n",
        "    <li><b>observation</b>: Observaciones del entorno</li>\n",
        "    <li><b>reward</b>: La recompensa que recogemos</li>\n",
        "    <li><b>done</b>: indica si hemos recogido o dejado un pasajero de forma exitosa, lo que denominaremos un <i>episodio</i></li>\n",
        "    <li><b>info</b>: Informacion adicional como la performance o la latencia de cara al debug</li>\n",
        "  </ul>\n",
        "  <li><code> entorno.render </code> Pinta un frame del entorno (muy útil para hacernos una idea del mismo)\n",
        "\n",
        "Nota adicional: hemos hecho <code>gym.make(\"Taxi-v3\").<b>env</b></code> para evitar que nos pare a las 200 iteraciones que es como funciona por defecto Gym\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOHJFvVBGa0-"
      },
      "source": [
        "### Hagamos memoria del problema\n",
        "\n",
        "Tenemos 4 localizaciones (con diferentes letras), y nuestro trabajo es recoger un pasajero en una de ellas y llevarlo a otra. Recibimos 20 puntos por una entrega exitosa, y perdemos 1 punto por cada paso de tiempo (para optimizar el recorrido). Hemos incorporado una penalización de 10 puntos por entregas o recogidas ilegales en localizaciones equivocadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwMYX--cDvlj",
        "outputId": "2cd9cae8-ff64-4bc1-87e6-b35fc1716a65"
      },
      "source": [
        "entorno.reset()\n",
        "entorno.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcTPKJFpHESy"
      },
      "source": [
        "Fijaros que ahora el taxi esta en otra localización.\n",
        "\n",
        "Más cosas del entorno:\n",
        "*   La cajita amarilla es el taxi\n",
        "*   Las paredes o setos por donde no hay carretera son los pipes (|)\n",
        "*   R, G, Y, B son los puntos de recogida y entrega. El color <font color=\"blue\"><b>azul</b></font> indica punto de recogida, el color <font color=\"purple\"><b>purpura</b></font> indica punto de entrega."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myIJE7y-HHll",
        "outputId": "32421320-501e-4631-e074-d4773027c0a9"
      },
      "source": [
        "print(\"Espacio acción {}\".format(entorno.action_space))\n",
        "print(\"Espacio estado {}\".format(entorno.observation_space))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Espacio acción Discrete(6)\n",
            "Espacio estado Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR8TtyBlINea"
      },
      "source": [
        "Fijaros que el Espacio Acción (action space) es de tamaño 6 y el Espacio Estado (state space) tiene tamaño 500. \n",
        "\n",
        "En terminos de mapeado que sepais que las acciones van de 0 a 5 con estos valores:\n",
        "\n",
        "*   0 = sur\n",
        "*   1 = norte\n",
        "*   2 = este\n",
        "*   3 = oeste\n",
        "*   4 = recoger pasajero\n",
        "*   5 = dejar pasajero\n",
        "\n",
        "Indice de puntos de recogida o entrega\n",
        "\n",
        "*   0 = R (0,0)\n",
        "*   1 = G (0,4)\n",
        "*   2 = Y (4,0)\n",
        "*   3 = B (4,3)\n",
        "\n",
        "Pero antes que nada, vamos a dar un vistazo a esa posición (3,1) que habiamos visto, veremos la información de ese estado. Con un pasajero esperando en la posición 2 (<font color=\"blue\"><b>Y</b></font>) con destino 0 (<font color=\"purple\"><b>R</b></font>)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mtXXvuRIsJ6",
        "outputId": "43f63685-a24f-4c61-8fbc-23e3629dcfce"
      },
      "source": [
        "estado = entorno.encode(3, 1, 2, 0) # (fila taxi, columa taxi, indice posicion pasajero, indice posición destino)\n",
        "print(\"Estado:\", estado)\n",
        "\n",
        "entorno.s = estado # Podemos asignar el estado actual al que queramos definir\n",
        "entorno.render()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estado: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUlzHFYZQT89"
      },
      "source": [
        "### La tabla de recompensas\n",
        "\n",
        "Com podeis ver de las 0 a la 499 coordenadas que tenemos en nuestro entorno, seria interesante si lo que hemos dispuesto en nuestras recompensas en la presentación es lo mismo que hay en Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U0l3Dg0QrmJ",
        "outputId": "c42da4af-991d-42bb-f4f6-6962a5935764"
      },
      "source": [
        "entorno.P[328]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGVbtB9GQq25"
      },
      "source": [
        "Los índices del diccionario de la posición 328 son las acciones a tomar desde es punto. Y siguen una estructura muy clara:\n",
        "\n",
        "<code>{acción: [(probabilidad, siguiente estado, recompensa, realizado)]}</code>\n",
        "\n",
        "Como podéis ver, en este entorno hemos asignado una <code>probabilidad</code> a cada acción del 100% (no hacemos distinciones, ni forzamos un comportamiento del agente.\n",
        "\n",
        "<code>siguiente estado</code> nos indica en que coordenadas terminariamos si tomaramos la acción del índice.\n",
        "\n",
        "La <code>recompensa</code> se muestra en esa tercera posición, con los valores de -1 si \"añadimos\" un paso de tiempo más y -10 si al taxi se le ocurre recoger o dejar a un pasajero. Y si miraramos en la coordenada de destino correcta con un pasajero dentro del taxi, aparecería un bonito 20 en <code>recompensa</code> en la acción 5 (dejar pasajero).\n",
        "\n",
        "La posición <code>realizado</code> se usa para decirnos que hemos dejado un pasajero en la localización correcta. Cada entrega existosa sera nuestro final de **episodio**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8euNv-wUsja"
      },
      "source": [
        "## 3. Que pasa si lo dejamos hacer solo al taxi (no Reinforcement Learning)\n",
        "\n",
        "Por el método de la fuerza bruta. Se trata de usar la tabla <code>P</code> de recompensas, que nos será la guia para la navegación del taxi.\n",
        "\n",
        "La idea es montar un loop infinito que no se parará hasta que el taxi deje a un pasajero en su destino (un simple **episodio**). O cuando recibamos una recompensa de 20. \n",
        "\n",
        "El método <code>entorno.action_space.sample()</code> toma una acción de forma aleatoria de todas las posibles acciones.\n",
        "\n",
        "Veamos que ocurre...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zXJgounQrKD",
        "outputId": "816eb46f-ada5-4d01-d682-a1a9bb80fd79"
      },
      "source": [
        "entorno.s = 328                 # coordenada inicio\n",
        "\n",
        "epochs = 0                      # inicial\n",
        "castigo, recompensa = 0, 0      # iniciales\n",
        "\n",
        "total_epochs = 0\n",
        "total_castigos = 0\n",
        "\n",
        "frames = [] # para la animación!!  ira acumulando frames\n",
        "\n",
        "finepisodio = False   # infinito\n",
        "\n",
        "while not finepisodio:\n",
        "    accion = entorno.action_space.sample()                       # info de entorno\n",
        "    estado, recompensa, finepisodio, info = entorno.step(accion) # recojo esta info\n",
        "\n",
        "    if recompensa == -10:\n",
        "        castigo += 1\n",
        "    \n",
        "    # Ponemos cada frame renderizado dentro de un diccionatio para la animación\n",
        "    frames.append({\n",
        "        'frame': entorno.render(mode='ansi'),\n",
        "        'episodio': 0,\n",
        "        'state': estado,\n",
        "        'action': accion,\n",
        "        'reward': recompensa\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "total_castigos += castigo\n",
        "total_epochs += epochs\n",
        "episodios = 1\n",
        "    \n",
        "print(\"Pasos de tiempo usados: {}\".format(epochs))\n",
        "print(\"Penalizaciones acumuladas: {}\".format(castigo))\n",
        "\n",
        "tiempomedio_random = total_epochs / episodios\n",
        "castigomedio_random = total_castigos / episodios\n",
        "\n",
        "print(f\"Resultados después de {episodios} episodios:\")\n",
        "print(f\"Media de tiempo por episodio: {tiempomedio_random}\")\n",
        "print(f\"Media de castigos por episodio: {castigomedio_random}\")\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pasos de tiempo usados: 188\n",
            "Penalizaciones acumuladas: 61\n",
            "Resultados después de 1 episodios:\n",
            "Media de tiempo por episodio: 188.0\n",
            "Media de castigos por episodio: 61.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPT777mCWoeY"
      },
      "source": [
        "### Pintemos los resultados en una bonita animación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oupumuHzWsYT",
        "outputId": "b24c451a-abc4-49e8-9dda-37b400804506"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep                          \n",
        "\n",
        "def pintar_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episodio: {frame['episodio']}\")\n",
        "        print(f\"Tiempo: {i + 1}\")\n",
        "        print(f\"Estado: {frame['state']}\")\n",
        "        print(f\"Accion: {frame['action']}\")\n",
        "        print(f\"Recompensa: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "\n",
        "pintar_frames(frames)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episodio: 0\n",
            "Tiempo: 188\n",
            "Estado: 0\n",
            "Accion: 5\n",
            "Recompensa: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SetUKvkIXzNH"
      },
      "source": [
        "Le ha costado la vida entregar al pasajero a su destino... no está mal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tzUDRkuZBTU"
      },
      "source": [
        "## 4. El turno de Q-Learning\n",
        "\n",
        "Recordemos que ahora hay que actualizar los valores de Q(a,s) en la Q-table, porque es de esa guía de donde sacaremos las mejores acciones para nuestro agente, el taxi.\n",
        "\n",
        "Valores más altos o mejores de Q indicaran para donde hemos de ir con nuestro taxi. Es decir, si el taxi se encuentra en un estado donde le espera un pasajero, es bastane probable que los valores para <code>recoger pasajero</code> sean mayores que el resto de acciones disponibles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vmwiCWuaeUj"
      },
      "source": [
        "### 4.1 Inicializando la Q-table a una matriz de 500x6 llena de ceros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNCa-UvDY_ot",
        "outputId": "ab99b76b-4961-4cf6-fd02-ba1944f7c625"
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([entorno.observation_space.n, entorno.action_space.n])\n",
        "\n",
        "print(q_table.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rhFQY1ubVTo"
      },
      "source": [
        "Fijamos el tiempo, y nos cargamos unas cuantas librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxMCXXDkbLoQ",
        "outputId": "7ec9b5f4-73cb-4785-8e11-ee61a4258c7e"
      },
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
            "Wall time: 11.2 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lh67D3xawhq"
      },
      "source": [
        "### 4.2 Montemos el código para el entrenamiento\n",
        "\n",
        "Definiremos los parámetros de actualización de la tabla Q (alpha, gamma) y el epsilon... ¡¡si nuestro amigo epsilon codicioso!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hefKWHPbbflY"
      },
      "source": [
        "alpha = 0.1 # Nuestro learning rate\n",
        "gamma = 0.6 # Nuestro descuento a la recompensa\n",
        "epsilon = 0.1 # Epsilon codicioso!!"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsqhS1KlbvCr"
      },
      "source": [
        "Definimos una variables para poder pintar metricas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX06aUU2bzaJ"
      },
      "source": [
        "all_epochs = []\n",
        "all_penalties = []"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2pAXSKab8Gy"
      },
      "source": [
        "Montamos el bucle de episodios, donde dentro meteremos el bucle del episodio en sí."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlg0bRlyb7gr",
        "outputId": "47c5bffe-ea53-48fa-db4c-ea6889263346"
      },
      "source": [
        "for i in range(1, 100001): # 100.000 episodios del modelo\n",
        "  estado = entorno.reset() # Arrancamos el entorno en una posición aleatoria cada vez\n",
        "\n",
        "  epochs, castigo, recompensa = 0, 0, 0\n",
        " \n",
        "  finepisodio = False       # no ay fin porque queremos que aprenda\n",
        "\n",
        "  while not finepisodio:    # Aquí va nuestro amigo Epsilon greedy, para fijar cuando explorarmos o explotamos\n",
        "    \n",
        "    if np.random.random() < epsilon:         # explorar, coger un dato aleatorio\n",
        "      accion = entorno.action_space.sample() # Exploramos el espacio de acciones\n",
        "    else: \n",
        "      accion = np.argmax(q_table[estado])    # Explotamos lo aprendido, sacar provecho de la tabla Q\n",
        "                                             # Filtrara por el estado y cogera la accion con valor de Q mas alto\n",
        "    siguiente_estado, recompensa, finepisodio, info = entorno.step(accion)\n",
        "\n",
        "    # Actualicemos los valores de la tabla Q en la posicion accion, estado tras ver lo que nos ha pasado\n",
        "    # Almacenamos el valor anterior de la tabla Q\n",
        "    valor_anterior = q_table[estado, accion]\n",
        "\n",
        "    # De la posicion estados, me quedo con el valor de la accion que daria el valor más alto en la tabla Q\n",
        "    siguiente_max = np.max(q_table[siguiente_estado])\n",
        "\n",
        "    # Calculamos la formulita de actualización de Q(a,s)\n",
        "    nuevo_valor = (1 - alpha) * valor_anterior + alpha * (recompensa + gamma * siguiente_max)\n",
        "    \n",
        "    # actualizamos el nuevo estado\n",
        "    q_table[estado, accion] = nuevo_valor\n",
        "\n",
        "    if recompensa == -10:\n",
        "      castigo += 1\n",
        "\n",
        "    estado = siguiente_estado\n",
        "    epochs += 1\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Episodio: {i}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Entrenamiento terminado.\\n\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodio: 100000\n",
            "Entrenamiento terminado.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlygjkf-hL65"
      },
      "source": [
        "Una vez terminado el entrenamiento, veamos los valores de la posición 328 para cada acción disponible en ese entorno según lo aprendido por el taxi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92JnP0fqhKWN",
        "outputId": "40cec1ae-28d5-44a0-f2f7-8b6c81a15075"
      },
      "source": [
        "q_table[328]       # en el array vemos los valores en esa posicion de: (sur, norte, oeste, este, recojer, dejar)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.41035118,  -2.27325184,  -2.40116522,  -2.35738709,\n",
              "       -10.86741224, -10.67550498])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ4XPaW7tnsY"
      },
      "source": [
        "La acción 1 (ir al norte) es la que tiene mejor \"recompensa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBK0u5jFhfh9"
      },
      "source": [
        "### 4.3 Evaluemos que tal lo hace nuestro agente\n",
        "\n",
        "Ahora ya no exploramos más, así que la parte de Epsilon Greedy la quitamos y tiramos directamente de los valores de Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSwwJRU6htVJ",
        "outputId": "67e36b3d-bd41-4100-a4d4-1350be1f623f"
      },
      "source": [
        "total_epochs, total_castigos = 0, 0\n",
        "episodios = 100\n",
        "\n",
        "frames = [] # para la animación!!\n",
        "\n",
        "for i in range(episodios):\n",
        "    estado = entorno.reset()\n",
        "    epochs, castigo, recompensa = 0, 0, 0\n",
        "    \n",
        "    finepisodio = False\n",
        "    \n",
        "    while not finepisodio:\n",
        "        accion = np.argmax(q_table[estado])\n",
        "        estado, recompensa, finepisodio, info = entorno.step(accion)\n",
        "\n",
        "        if recompensa == -10:\n",
        "          castigo += 1\n",
        "        \n",
        "        epochs += 1\n",
        "         \n",
        "\n",
        "        # Ponemos cada frame renderizado dentro de un diccionatio para la animación\n",
        "        frames.append({\n",
        "          'frame': entorno.render(mode='ansi'),\n",
        "          'episodio': i,\n",
        "          'state': estado,\n",
        "          'action': accion,\n",
        "          'reward': recompensa\n",
        "          }\n",
        "        )\n",
        "\n",
        "    total_castigos += castigo\n",
        "    total_epochs += epochs\n",
        "\n",
        "\n",
        "\n",
        "tiempomedio_qlearning = total_epochs / episodios\n",
        "castigomedio_qlearning = total_castigos / episodios\n",
        "\n",
        "print(f\"Resultados después de {episodios} episodios:\")\n",
        "print(f\"Media de tiempo por episodio: {tiempomedio_qlearning}\")\n",
        "print(f\"Media de castigos por episodio: {castigomedio_qlearning}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resultados después de 100 episodios:\n",
            "Media de tiempo por episodio: 12.9\n",
            "Media de castigos por episodio: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al5_VsPki5Qy",
        "outputId": "a1d8a5a0-51fe-4f06-d3e2-e60d3a54d06c"
      },
      "source": [
        "pintar_frames(frames)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episodio: 99\n",
            "Tiempo: 1290\n",
            "Estado: 475\n",
            "Accion: 5\n",
            "Recompensa: 20\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}