{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rnn_textgeneration_keras_gru_quijote_ds1020str.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidPuigV/Nuclio/blob/main/rnn_textgeneration_keras_gru_quijote_ds1020str.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxinyzWktitF"
      },
      "source": [
        "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0iCRShPATnu"
      },
      "source": [
        "## Generación de texto mediante redes neuronales recurrentes\n",
        "\n",
        "Un ejemplo montado en Keras para RNN usando una GRU con un texto de Quijote"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7vN5sT0ATnx"
      },
      "source": [
        "# 1. Librerias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX-Tek_3wQIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5443e440-bf5c-4e38-9e12-910be931b452"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from tensorflow import keras as ks\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "!pip install unidecode\n",
        "import unidecode"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uhDEuEMATn5"
      },
      "source": [
        "# 2. Cargamos los datos - el corpus basado en textos de Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEIxYXPGujCA",
        "outputId": "e72cc0c7-41c5-4e8c-a291-c8d7be00e417"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr303fyeAToC"
      },
      "source": [
        "Preparamos los datos haciendo algunas manipulaciones, como la primera, decodificar el texto que viene en Unicode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtYfNl4-0Pg6"
      },
      "source": [
        "def preproceso(text):\n",
        "  text = unidecode.unidecode(text)\n",
        "  text = text.replace('\\n', ' ').replace('\\ufeff', '').lower() # nos cargamos saltos y lo pasamos todo a minuscula\n",
        "  text = ''.join(x for x in text if x not in \"%&$#=<>/*+@][\")  # nos cargamos caracteres extranos\n",
        "  while len(text) != len(text.replace(\"  \", \" \")):  # aqui hacemos un replace de muchos espacios\n",
        "    text = text.replace(\"  \", \" \")\n",
        "  return text\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S6_W2CMW34e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d58e131-8fdb-4916-9ac6-02e668d12720"
      },
      "source": [
        "with open(\"./quijote.txt\", 'r') as infile:\n",
        "  text = preproceso(infile.read())   # aplicamos el preproceso y limpiamos el texto, vemos longitud y vocabulario de 48 caracteres unicos\n",
        "\n",
        "print ('Longitud del corpus: {} caracteres'.format(len(text)))\n",
        "print ('Ejemplo de texto...')\n",
        "print(text[:250])\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "print ('{} caracteres unicos'.format(len(vocab)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longitud del corpus: 2106939 caracteres\n",
            "Ejemplo de texto...\n",
            "el ingenioso hidalgo don quijote de la mancha tasa yo, juan gallo de andrada, escribano de camara del rey nuestro senor, de los que residen en su consejo, certifico y doy fe que, habiendo visto por los senores del un libro intitulado el ingenioso hid\n",
            "[' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "48 caracteres unicos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x5h9dJcAToG"
      },
      "source": [
        "# 3. Pre-proceso de los datos\n",
        "\n",
        "## 3.1 Indexado de carácteres\n",
        "Montamos un indice para los carácteres, para tener valores numéricos, y vemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rZMLuuUXTsQ"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}    # caracteres a indices y hacemos la inversa\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da2gp8w3Xg1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271070ad-a14d-4bd7-bb28-d0bc61e12d78"
      },
      "source": [
        "print('{')   # esto es lo que hemos hecho transformando letras por numeros\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "print ('{} ---- carácteres mapeados a números enteros ---- > {}'.format(repr(text[:13]), text_as_int[:13]))   # asi queda el ingenioso"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  ' ' :   0,\n",
            "  '!' :   1,\n",
            "  '\"' :   2,\n",
            "  \"'\" :   3,\n",
            "  '(' :   4,\n",
            "  ')' :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '0' :   9,\n",
            "  '1' :  10,\n",
            "  '2' :  11,\n",
            "  '3' :  12,\n",
            "  '4' :  13,\n",
            "  '5' :  14,\n",
            "  '6' :  15,\n",
            "  '7' :  16,\n",
            "  '8' :  17,\n",
            "  '9' :  18,\n",
            "  ':' :  19,\n",
            "  ...\n",
            "}\n",
            "'el ingenioso ' ---- carácteres mapeados a números enteros ---- > [26 33  0 30 35 28 26 35 30 36 40 36  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vny-_e9AToR"
      },
      "source": [
        "Fijamos cada texto en secuencias de 100 carácteres, y mostramos como se va a ir entregando la inforamción a la red neuronal recurrente..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKniYUAlXo4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b7e745-6d31-4530-ddc3-69b3f4da9076"
      },
      "source": [
        "seq_length = 100     # aprenderemos de secuencias de 100 caracteres, aqui lo tenemos como un array\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(10):\n",
        "  print(idx2char[i.numpy()])\n",
        "\n",
        "print(char_dataset)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "e\n",
            "l\n",
            " \n",
            "i\n",
            "n\n",
            "g\n",
            "e\n",
            "n\n",
            "i\n",
            "o\n",
            "<TensorSliceDataset shapes: (), types: tf.int64>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEfUnFusAToW"
      },
      "source": [
        "## 3.2 Definimos las secuencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybj51Zi0X3V-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3132ccdd-c089-4f7c-bbfc-66af86703201"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)   # aqui tenemos las primeras 5 secuancias para entrenar mi red neuronal\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))     "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'el ingenioso hidalgo don quijote de la mancha tasa yo, juan gallo de andrada, escribano de camara del'\n",
            "' rey nuestro senor, de los que residen en su consejo, certifico y doy fe que, habiendo visto por los '\n",
            "'senores del un libro intitulado el ingenioso hidalgo de la mancha, compuesto por miguel de cervantes '\n",
            "'saavedra, tasaron cada pliego del dicho libro a tres maravedis y medio; el cual tiene ochenta y tres '\n",
            "'pliegos, que al dicho precio monta el dicho libro docientos y noventa maravedis y medio, en que se ha'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDAb-M7AX8HF"
      },
      "source": [
        "def split_input_target(chunk):    # las decalamos 1 caracter por delante y por detras\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKw_n3b1YAeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e2008c-dba3-43af-af27-ab99694e88a3"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):       # las decalamos 1 caracter por delante y por detras\n",
        "  print ('Secuencia de entrada: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Secuencia de salida:  ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Secuencia de entrada:  'el ingenioso hidalgo don quijote de la mancha tasa yo, juan gallo de andrada, escribano de camara de'\n",
            "Secuencia de salida:    'l ingenioso hidalgo don quijote de la mancha tasa yo, juan gallo de andrada, escribano de camara del'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfrKI_61YErq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afe8dac-4651-445b-82ad-bcc54d5b33e1"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):   # aqui tenemos los pasos, estados de la red neuronal\n",
        "    print(\"Paso {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  output esperado: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Paso    0\n",
            "  input: 26 ('e')\n",
            "  output esperado: 33 ('l')\n",
            "Paso    1\n",
            "  input: 33 ('l')\n",
            "  output esperado: 0 (' ')\n",
            "Paso    2\n",
            "  input: 0 (' ')\n",
            "  output esperado: 30 ('i')\n",
            "Paso    3\n",
            "  input: 30 ('i')\n",
            "  output esperado: 35 ('n')\n",
            "Paso    4\n",
            "  input: 35 ('n')\n",
            "  output esperado: 28 ('g')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rime3ERQATor"
      },
      "source": [
        "## 3.3 Montamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUTxvrDtYKJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59086ea8-f2f6-4f12-b73c-935792ff45da"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISd67WS7AToz"
      },
      "source": [
        "# 4. Montamos la red neuronal recurrente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW4s_v6FYuCv"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256     # longitud del vector\n",
        "rnn_units = 1024     # 1024 unidaddes de GRU"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7qCpfNXY6yv"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = ks.Sequential()\n",
        "    model.add(ks.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]))\n",
        "    model.add(ks.layers.GRU(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True))\n",
        "    model.add(ks.layers.Dense(vocab_size))  # nos da probabilidades\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyttS9FZY90d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ac2a2e-996a-40d6-a38d-16e31e855d7a"
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bw-Lnu-ZItl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04722e01-80d3-4e86-880f-10c723c9497d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           12288     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 48)            49200     \n",
            "=================================================================\n",
            "Total params: 3,999,792\n",
            "Trainable params: 3,999,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5x-oGoFFXg"
      },
      "source": [
        "# 5. Cogemos el modelo con pesos aleatorios que hemos creado y generamos una primera predicción con el mismo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsoEMUCuZA3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a250a739-99ad-4b61-9cf8-59f19fa08b33"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):     # montamos los tensores de calculo para meter dentro la red neuronal\n",
        "    print(input_example_batch)\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[21 38 42 ... 37 22 39]\n",
            " [ 0 38 42 ...  6  0 37]\n",
            " [40 36  6 ... 47  0 37]\n",
            " ...\n",
            " [26 39 36 ... 38 42 26]\n",
            " [22  0 33 ... 33 22 36]\n",
            " [41 36 39 ...  3  3  0]], shape=(64, 100), dtype=int64)\n",
            "(64, 100, 48) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8O5ec4jZNU1"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUQ_LMQZZRbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60a6eb5-2e13-4911-9832-cfb32aaf5d02"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 26, 15, 12,  0, 25, 40,  0, 45, 18, 20, 14, 20, 14, 26, 39, 22,\n",
              "       21, 11, 11, 45, 26,  5, 12,  9, 43, 41, 20, 35, 44, 12, 10,  2, 34,\n",
              "        3, 45, 47, 27,  1, 13, 47, 34, 36,  5, 19,  5, 10, 17, 36, 19,  1,\n",
              "       29, 10, 18,  3, 10,  2, 15, 41, 18, 43, 23, 30, 16,  1, 37, 40, 15,\n",
              "       47, 33, 13,  9, 26, 44,  4, 45, 34, 42, 43, 26, 32, 22, 32, 43,  4,\n",
              "       12, 21, 20, 13, 16, 12, 36,  7,  9,  9, 31,  5, 12, 44, 46])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-VHjgdQZUYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5915b4a6-68fd-4a84-8039-c5433e592c95"
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " '?que locura es esta? mire que no hay gigante ni caballero alguno, ni gatos, ni armas, ni escudos par'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'we63 ds x9;5;5era?22xe)30vt;nw31\"m\\'xzf!4zmo):)18o:!h19\\'1\"6t9vbi7!ps6zl40ew(xmuvekakv(3?;473o-00j)3wy'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjCfTPPWF69y"
      },
      "source": [
        "# 6. Creamos una funcion de perdida loss\n",
        "\n",
        "La función de pérdida estándar <code>tf.keras.losses.sparse_categorical_crossentropy</code> funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
        "\n",
        "Debido a que nuestro modelo devuelve logits, necesitamos establecer la <code>from_logits</code>. Logits son un conjunto de probabilidades sin scalar, es decir, que no hay que meter ningún \"softmax\" en la salida de la red neuronal.\n",
        "\n",
        "Aprovechamos y calculamos el error escalar que estamos cometiendo en la predicción que hemos hecho en la anterior etapa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLE08XdsZZSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1de9a8-f271-48a4-b4c9-5ffeddc02601"
      },
      "source": [
        "def loss(labels, logits):     # definimos la funcion de perdidas\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 48)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       3.8740597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN0gcaUGG1LZ"
      },
      "source": [
        "# 7. Compilamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axspBr2Zexm"
      },
      "source": [
        "model.compile(\n",
        "    optimizer = \"adam\",\n",
        "    loss = loss,\n",
        "    metrics = ['accuracy'])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOugHVUEG7Db"
      },
      "source": [
        "# 8. Definimos checkpoints donde almacenar los modelos a cada epoch, usando callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7LsmraVZlCK"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=ks.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mc-Ny-qHFB0"
      },
      "source": [
        "# 9. Entrenamos el modelo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alxAisExZoUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abc0911-8612-4180-ed4b-3b63e651e9c9"
      },
      "source": [
        "EPOCHS=10\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "329/329 [==============================] - 27s 76ms/step - loss: 2.5165 - accuracy: 0.2890\n",
            "Epoch 2/10\n",
            "329/329 [==============================] - 26s 78ms/step - loss: 1.5936 - accuracy: 0.4984\n",
            "Epoch 3/10\n",
            "329/329 [==============================] - 26s 78ms/step - loss: 1.3600 - accuracy: 0.5687\n",
            "Epoch 4/10\n",
            "329/329 [==============================] - 26s 79ms/step - loss: 1.2676 - accuracy: 0.5947\n",
            "Epoch 5/10\n",
            "329/329 [==============================] - 26s 80ms/step - loss: 1.2125 - accuracy: 0.6104\n",
            "Epoch 6/10\n",
            "329/329 [==============================] - 26s 80ms/step - loss: 1.1761 - accuracy: 0.6202\n",
            "Epoch 7/10\n",
            "329/329 [==============================] - 26s 80ms/step - loss: 1.1434 - accuracy: 0.6301\n",
            "Epoch 8/10\n",
            "329/329 [==============================] - 26s 80ms/step - loss: 1.1151 - accuracy: 0.6389\n",
            "Epoch 9/10\n",
            "329/329 [==============================] - 26s 79ms/step - loss: 1.0873 - accuracy: 0.6470\n",
            "Epoch 10/10\n",
            "329/329 [==============================] - 26s 79ms/step - loss: 1.0626 - accuracy: 0.6552\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5jyuPm_QN9V"
      },
      "source": [
        "# 10. Reconstruimos el modelo con los pesos entrenados\n",
        "\n",
        "Debido a la forma en que el estado RNN se pasa de un paso de tiempo a otro, el modelo solo acepta un tamaño de batch fijo una vez construido.\n",
        "\n",
        "Para ejecutar el modelo con un batch_size diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chRUagh2ZxPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00afa410-8f45-4c22-d724-f8575bb37b4a"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            12288     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 48)             49200     \n",
            "=================================================================\n",
            "Total params: 3,999,792\n",
            "Trainable params: 3,999,792\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56_sp1guQ0uO"
      },
      "source": [
        "# 11.Montamos una función para generar texto\n",
        "\n",
        "Consiste en que iteremos generando 1000 caracteres a partir de una semilla definida.\n",
        "\n",
        "Existe un parametro llamado **temperatura** que lo modificaremos para ver los resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWNak8zYZ_2-"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  temperature = 0.5\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "      if idx2char[predicted_id] == \",\":\n",
        "        text_generated.append('\\n')\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYoUN9Utiti"
      },
      "source": [
        "# 12. Lanzamos nuestra predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIqNtAUUREpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6bc5fe7-1ade-4548-b2c5-651d6135e89e"
      },
      "source": [
        "print(generate_text(model, start_string=u\"dulcinea \"))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dulcinea en las salas penas,\n",
            " sancho,\n",
            " que me diga a vuestra merced a vuestra merced alguna cosa que la hermosura de la hija de aquella belleza de mi cuerpo de la insolencia de su senor,\n",
            " y para el cual podia ser que el cielo hizo de la mancha la sangre se vinieron a caballo y a decir mas cuantos lo primero. -eso no -respondio sancho-,\n",
            " porque si el cielo me lo ha querido dar a la cabeza y van a un hombre a los caballeros andantes han de saber hacer lo que tanto despues a caballo se le ofrecio a las manos a la caballeriza,\n",
            " se quedo de alli a manara de las armas de tan cortesano a la cabeza con las armas,\n",
            " las cuales bastaban de pedro a los amantes,\n",
            " la deshonestidad y recatada,\n",
            " porque a su senor tio de la enfermedad de que sea aquella noche que llegaban compania en las manos,\n",
            " como si dijeren en ellos un poco a la venta,\n",
            " que estaba en la cabeza en su amo que tantas veces la habia de alabar al buen pario de mano a la senora dulcinea,\n",
            " y que le habia puesto la cabeza con las alas del contento de su amo,\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}